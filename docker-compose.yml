services:
  app:
    build:
      context: .
      dockerfile: Dockerfile
      args:
        WITH_CUDA: ${WITH_CUDA:-true}
        CUDA_ARCHS: ${CUDA_ARCHS:-120}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    container_name: ask_llm_app
    env_file:
      - ${HOME}/.config/ask-llm/.env
    environment:
      # Database connection (external postgres.home server)
      ASK_LLM_POSTGRES_HOST: ${POSTGRES_HOST:-postgres.home}
      ASK_LLM_POSTGRES_PORT: ${POSTGRES_PORT:-5432}
      ASK_LLM_POSTGRES_DATABASE: ${POSTGRES_DB:-askllm}
      ASK_LLM_POSTGRES_USER: ${POSTGRES_USER:-askllm}
      ASK_LLM_POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-askllm_mem0ry}

      # Service configuration
      ASK_LLM_MEMORY_SERVER_HOST: 0.0.0.0
      ASK_LLM_MEMORY_SERVER_PORT: 8001
      ASK_LLM_SERVICE_HOST: 0.0.0.0
      ASK_LLM_SERVICE_PORT: 8642

      # Memory server URL (internal)
      ASK_LLM_MEMORY_SERVER_URL: http://127.0.0.1:8001

      # API Keys (pass through from host .env)
      OPENAI_API_KEY: ${OPENAI_API_KEY:-}
      ANTHROPIC_API_KEY: ${ANTHROPIC_API_KEY:-}
      TAVILY_API_KEY: ${TAVILY_API_KEY:-}
      HF_TOKEN: ${HF_TOKEN:-}
      HUGGINGFACEHUB_API_TOKEN: ${HF_TOKEN:-}

      # Hugging Face / Transformers logging control
      HF_HUB_DISABLE_PROGRESS_BARS: ${HF_HUB_DISABLE_PROGRESS_BARS:-1}
      TRANSFORMERS_NO_ADVISORY_WARNINGS: ${TRANSFORMERS_NO_ADVISORY_WARNINGS:-1}
      TRANSFORMERS_VERBOSITY: ${TRANSFORMERS_VERBOSITY:-error}
      TOKENIZERS_PARALLELISM: ${TOKENIZERS_PARALLELISM:-false}

      # User and bot settings
      ASK_LLM_DEFAULT_USER: ${DEFAULT_USER:-nick}
      ASK_LLM_DEFAULT_BOT: ${ASK_LLM_DEFAULT_BOT:-nova}
      ASK_LLM_DEFAULT_MODEL_ALIAS: ${ASK_LLM_DEFAULT_MODEL_ALIAS:-gpt-5.2-2025-12-11}

      # Nextcloud Talk bot (from ~/.config/ask-llm/.env)
      ASK_LLM_NEXTCLOUD_BOT_SECRET: ${ASK_LLM_NEXTCLOUD_BOT_SECRET:-}
      ASK_LLM_NEXTCLOUD_URL: ${ASK_LLM_NEXTCLOUD_URL:-https://nextcloud.ferreri.us}

      # Logging
      ASK_LLM_LOG_PREFIX: ${ASK_LLM_LOG_PREFIX:-docker}
      ASK_LLM_DEBUG_TURN_LOG: ${ASK_LLM_DEBUG_TURN_LOG:-true}
      ASK_LLM_LOG_DIR: /app/.logs
    ports:
      - "${MCP_PORT:-8001}:8001"
      - "${SERVICE_PORT:-8642}:8642"
    volumes:
      # Mount host config directory to avoid duplication
      - ${HOME}/.config/ask-llm:/root/.config/ask-llm
      # Mount logs directory
      - ./.logs:/app/.logs
      # Mount bots.yaml for live updates (nextcloud provisioning, etc.)
      - ./src/ask_llm/bots.yaml:/app/src/ask_llm/bots.yaml:ro
      # Optional: mount local models
      - ${MODELS_PATH:-./models}:/app/models
      # Share host's GGUF model cache to avoid re-downloads
      - ${ASK_LLM_MODEL_CACHE_DIR:-${HOME}/.cache/ask_llm/models}:/root/.cache/ask_llm/models
      # Mount HuggingFace cache for GGUF models
      - ${HF_HOME:-${HOME}/.cache/huggingface}:/root/.cache/huggingface
    restart: unless-stopped
    stdin_open: true
    tty: true
