[project]
name = "ask_llm"
version = "0.1.0"
description = "Ask LLM is a Python-based application that allows users to interact with various language models, including OpenAI and Ollama. The application provides a command-line interface for querying these models and managing conversation history."
readme = "README.md"
requires-python = ">=3.12"

dependencies = [
    "rich>=13.9.4",
    "requests>=2.25.0",
    "pydantic-settings>=2.8.1",
    "openai>=1.69.0",
    "pyyaml>=6.0.2",
    "pytz>=2025.2",
    "tiktoken>=0.9.0",
    # PostgreSQL memory backend
    "sqlalchemy>=2.0.0",
    "sqlmodel>=0.0.31",
    "psycopg2-binary>=2.9.0",
    "pgvector>=0.3.0",
]

[project.optional-dependencies]
dev = [
    "ruff",
    "mypy",
    "pytest>=8.3.5",
    "pytest-cov>=5.0.0",
    "pytest-mock>=3.14.0",
]
memory = [
    "sentence-transformers>=2.2.0",
]
huggingface = [
    "transformers>=4.35.0",
    "bitsandbytes>=0.42.0",
    "torch>=2.2.0",
    "accelerate>=0.29.0",
    "peft>=0.5.0",
    "xformers>=0.0.29.post1",
    "huggingface-hub"
]

llamacpp = [
    "llama-cpp-python",
]

service = [
    "fastapi>=0.109.0",
    "uvicorn[standard]>=0.27.0",
    "httpx>=0.26.0",
]

mcp = [
    "mcp[cli]>=1.0.0",
]


[project.scripts]
ask-llm = "ask_llm.main:main"
llm = "ask_llm.main:main"
ask-llm-service = "ask_llm.service.server:main"
llm-service = "ask_llm.service.server:main"
llm-memory = "ask_llm.memory_debug:main"
llm-mcp-server = "ask_llm.memory_server:run_server"

[project.entry-points."ask_llm.memory"]
postgresql = "ask_llm.memory.postgresql:PostgreSQLMemoryBackend"

[tool.setuptools]
package-dir = {"" = "src"}
include-package-data = true

[tool.setuptools.package-data]
ask_llm = ["*.yaml"]

[build-system]
requires = ["setuptools>=61.0"]
build-backend = "setuptools.build_meta"

[tool.setuptools.packages.find]
where = ["src"]
